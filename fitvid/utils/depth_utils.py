import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F
import torch


DEFAULT_WEIGHT_LOCATIONS = {
    "dpt": "/viscam/u/stian/perceptual-metrics/MiDaS/weights/dpt_hybrid-midas-501f0c75.pt",
    "mn": "/viscam/u/stian/perceptual-metrics/MiDaS/weights/midas_v21-f6b98070.pt",
    "mns": "/viscam/u/stian/perceptual-metrics/MiDaS/weights/midas_v21_small-70d6b9c8.pt",
}


def normalize_depth(t, across_dims=1):
    # Normalize depth values across all but the first dimension.
    # A 4-D input will be normalized per frame, but a 5-D input will be normalized per sequence.
    dims = tuple(range(len(t.shape))[across_dims:])
    t = t - t.amin(dim=dims, keepdim=True)
    t = t / (t.amax(dim=dims, keepdim=True) + 1e-10)
    return t


def normalize_depth_npy(t, across_dims=1):
    # Normalize depth values across all but the first dimension.
    # A 4-D input will be normalized per frame, but a 5-D input will be normalized per sequence.
    dims = tuple(range(len(t.shape))[across_dims:])
    t = t - np.amin(t, axis=dims, keepdims=True)
    t = t / (np.amax(t, axis=dims, keepdims=True) + 1e-10)
    return t


def depth_to_rgb_im(im, cmap=plt.get_cmap("jet_r")):
    # shape = [(T), 1, W, H]
    if len(im.shape) >= 3 and im.shape[-3] == 1:
        im = np.squeeze(im, axis=-3)
    if im.shape[-1] == 1:
        im = np.squeeze(im, axis=-1)
    # convert to rgb using given cmap
    im = cmap(im)[..., :3]
    return (im * 255).astype(np.uint8)


def sobel_loss(t1, t2, reduce_batch=True):
    from kornia.filters import sobel

    original_shape = t1.shape
    compress_shape = (-1,) + tuple(t1.shape[-3:])  # change to (B, C, H, W)
    t1, t2 = torch.reshape(t1, compress_shape), torch.reshape(t2, compress_shape)
    t1_sobel, t2_sobel = sobel(t1), sobel(t2)
    t1_sobel, t2_sobel = torch.reshape(t1_sobel, original_shape), torch.reshape(
        t2_sobel, original_shape
    )
    return mse_loss(t1_sobel, t2_sobel, reduce_batch=reduce_batch)


def pixel_wise_loss(t1, t2, loss="l2", reduce_batch=True, mask=None):
    if loss == "l2":
        pwl = (t1 - t2) ** 2
    elif loss == "l1":
        pwl = torch.abs(t1 - t2)
    else:
        raise NotImplementedError("Only l2 and l1 losses are implemented")

    if mask is not None:
        pwl = mask * pwl
    if reduce_batch:
        return pwl.mean()
    else:
        reduce_dims = tuple(range(len(t1.shape)))
        return pwl.mean(dim=reduce_dims[1:])


def pixel_wise_loss_segmented(t1, t2, segmentation, loss="l2", reduce_batch=True):
    mask = torch.ones_like(t2).type(torch.uint8)
    segmentation = torch.tile(segmentation, (1, 1, 3, 1, 1))
    mask[segmentation != 1] = 0
    mask[segmentation == 1] = 1
    pwl = pixel_wise_loss(t1, t2, loss=loss, reduce_batch=reduce_batch, mask=mask)
    return pwl


def test_mse_loss():
    t1, t2 = torch.randn((30, 20, 10)), torch.randn((30, 20, 10))
    my_mse = mse_loss(t1, t2, reduce_batch=True)
    torch_mse = torch.nn.functional.mse_loss(t1, t2)
    print(f"Mine: {my_mse}, nn.Functional: {torch_mse}")
    assert torch.allclose(my_mse, torch_mse), "Reducing all dimensions doesn't match"
    per_sample_mse = mse_loss(t1, t2, reduce_batch=False)
    assert torch.allclose(
        torch_mse, per_sample_mse.mean()
    ), "Not reducing yields different results"
    print(f"Mine: {per_sample_mse.mean()}, nn.Functional: {torch_mse}")


if __name__ == "__main__":
    test_mse_loss()
